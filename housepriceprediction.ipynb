{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi==0.63.0 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (0.63.0)\n",
      "Requirement already satisfied: uvicorn==0.13.3 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: pytest==6.2.4 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (6.2.4)\n",
      "Requirement already satisfied: requests==2.25.1 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (2.25.1)\n",
      "Requirement already satisfied: pandas==1.2.4 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn==0.24.1 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (0.24.1)\n",
      "Requirement already satisfied: numpy==1.19.2 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (1.19.2)\n",
      "Requirement already satisfied: matplotlib==3.3.2 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (3.3.2)\n",
      "Requirement already satisfied: gdown==3.13.0 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (3.13.0)\n",
      "Requirement already satisfied: Flask==1.1.2 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (1.1.2)\n",
      "Requirement already satisfied: wtforms==2.3.3 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (2.3.3)\n",
      "Requirement already satisfied: findspark==1.4.2 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (1.4.2)\n",
      "Requirement already satisfied: pyspark==3.1.2 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (3.1.2)\n",
      "Requirement already satisfied: uuid==1.30 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 15)) (1.30)\n",
      "Requirement already satisfied: confluent_kafka==1.7.0 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 16)) (1.7.0)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.0.0 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from fastapi==0.63.0->-r requirements.txt (line 1)) (1.8.2)\n",
      "Requirement already satisfied: starlette==0.13.6 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from fastapi==0.63.0->-r requirements.txt (line 1)) (0.13.6)\n",
      "Requirement already satisfied: click==7.* in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from uvicorn==0.13.3->-r requirements.txt (line 2)) (7.1.2)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from uvicorn==0.13.3->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pytest==6.2.4->-r requirements.txt (line 3)) (20.3.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pytest==6.2.4->-r requirements.txt (line 3)) (0.13.1)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pytest==6.2.4->-r requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pytest==6.2.4->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pytest==6.2.4->-r requirements.txt (line 3)) (0.4.4)\n",
      "Requirement already satisfied: atomicwrites>=1.0; sys_platform == \"win32\" in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pytest==6.2.4->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pytest==6.2.4->-r requirements.txt (line 3)) (20.4)\n",
      "Requirement already satisfied: toml in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pytest==6.2.4->-r requirements.txt (line 3)) (0.10.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from requests==2.25.1->-r requirements.txt (line 4)) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from requests==2.25.1->-r requirements.txt (line 4)) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from requests==2.25.1->-r requirements.txt (line 4)) (2020.6.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from requests==2.25.1->-r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pandas==1.2.4->-r requirements.txt (line 5)) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pandas==1.2.4->-r requirements.txt (line 5)) (2.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from scikit-learn==0.24.1->-r requirements.txt (line 6)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from scikit-learn==0.24.1->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from scikit-learn==0.24.1->-r requirements.txt (line 6)) (0.17.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from matplotlib==3.3.2->-r requirements.txt (line 9)) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from matplotlib==3.3.2->-r requirements.txt (line 9)) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from matplotlib==3.3.2->-r requirements.txt (line 9)) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from matplotlib==3.3.2->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: six in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from gdown==3.13.0->-r requirements.txt (line 10)) (1.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from gdown==3.13.0->-r requirements.txt (line 10)) (3.0.12)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from gdown==3.13.0->-r requirements.txt (line 10)) (4.50.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from Flask==1.1.2->-r requirements.txt (line 11)) (1.0.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from Flask==1.1.2->-r requirements.txt (line 11)) (2.11.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from Flask==1.1.2->-r requirements.txt (line 11)) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from wtforms==2.3.3->-r requirements.txt (line 12)) (1.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pyspark==3.1.2->-r requirements.txt (line 14)) (0.10.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\saipa\\anaconda3\\lib\\site-packages (from pydantic<2.0.0,>=1.0.0->fastapi==0.63.0->-r requirements.txt (line 1)) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "# Install Requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - confluent_kafka\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/win-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/win-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n"
     ]
    }
   ],
   "source": [
    "conda install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing cimpl: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-821b4c830741>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mconfluent_kafka\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProducer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\confluent_kafka\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdeserializing_consumer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDeserializingConsumer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mserializing_producer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSerializingProducer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKafkaException\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKafkaError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\confluent_kafka\\deserializing_consumer.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mconfluent_kafka\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcimpl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConsumer\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_ConsumerImpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m from .error import (ConsumeError,\n\u001b[0;32m     21\u001b[0m                     \u001b[0mKeyDeserializationError\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing cimpl: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import random\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka producer variables\n",
    "\n",
    "simple_messages = [\n",
    "'I love this pony',\n",
    "'This restaurant is great',\n",
    "'The weather is bad today',\n",
    "'I will go to the beach this weekend',\n",
    "'She likes to swim',\n",
    "'Apple is a great company'\n",
    "]\n",
    "\n",
    "bootstrap_servers='127.0.0.1:9092'\n",
    "topic='test'\n",
    "msg_count=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_report(err, msg):\n",
    "    \"\"\" Called once for each message produced to indicate delivery result.\n",
    "        Triggered by poll() or flush(). \"\"\"\n",
    "    if err is not None:\n",
    "        print('Message delivery failed: {}'.format(err))\n",
    "    else:\n",
    "        print('Message delivered to {}'.format(msg.topic()))\n",
    "\n",
    "def confluent_kafka_producer():\n",
    "\n",
    "    p = Producer({'bootstrap.servers': bootstrap_servers})\n",
    "    for data in simple_messages:\n",
    "        \n",
    "        record_key = str(uuid.uuid4())\n",
    "        record_value = json.dumps({'data': data})\n",
    "        \n",
    "        p.produce(topic, key=record_key, value=record_value, on_delivery=delivery_report)\n",
    "        p.poll(0)\n",
    "\n",
    "    p.flush()\n",
    "    print('we\\'ve sent {count} messages to {brokers}'.format(count=len(simple_messages), brokers=bootstrap_servers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confluent_kafka_producer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('RealtimeKafkaML') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark \\\n",
    "  .readStream \\\n",
    "  .format('kafka') \\\n",
    "  .option('kafka.bootstrap.servers', bootstrap_servers) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option('subscribe', topic) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = df_raw.selectExpr('CAST(value AS STRING) as json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a small batch of data from kafka and display to the console\n",
    "\n",
    "schema = StructType([StructField('data', StringType())])\n",
    "\n",
    "df_json.select(from_json(df_json.json, schema).alias('raw_data')) \\\n",
    "  .select('raw_data.data') \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .format(\"console\") \\\n",
    "  .start() \\\n",
    "  .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentiment_analysis(data):\n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    result = requests.post('http://localhost:9000/predict', json=json.loads(data))\n",
    "    return json.dumps(result.json())\n",
    "\n",
    "vader_udf = udf(lambda data: apply_sentiment_analysis(data), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema_input = StructType([StructField('data', StringType())])\n",
    "schema_output = StructType([StructField('neg', StringType()),\\\n",
    "                            StructField('pos', StringType()),\\\n",
    "                            StructField('neu', StringType()),\\\n",
    "                            StructField('compound', StringType())])\n",
    "\n",
    "df_json.select(from_json(df_json.json, schema_input).alias('sentence'),\\\n",
    "               from_json(vader_udf(df_json.json), schema_output).alias('response'))\\\n",
    "  .select('sentence.data', 'response.*') \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .format(\"console\") \\\n",
    "  .start() \\\n",
    "  .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
