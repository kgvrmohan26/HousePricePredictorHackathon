
Download Links/commands:
For Kafka : wget https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz
For Java : sudo apt install openjdk-8-jdk -y
For kafka python : pip3 install kakfa-python
For Hadoop:  wget https://archive.apache.org/dist/hadoop/common/hadoop-2.8.5/hadoop-2.8.5.tar.gz
For SSH: sudo apt install openssh-server openssh-client -y
For Hive: wget http://archive.apache.org/dist/hive/hive-2.3.5/apache-hive-2.3.5-bin.tar.gz
For Spark: wget https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
For PySpark: pip3 install pyspark==2.4.6
For spark-streaming-kafka-0-8-assembly_2.11: https://search.maven.org/search?q=a:spark-streaming-kafka-0-8-assembly_2.11

After updating the environment variables use this comamnd to reflect the changes : source .bashrc
Update Hadoop environment variables: Edit .bashrc found in your home directory, and add the following.
	export HADOOP_HOME=/home/<USER>/hadoop
	export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export HADOOP_INSTALL=$HADOOP_HOME
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export YARN_HOME=$HADOOP_HOME
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
	export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
	export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

Update Hive environment variables: Add the following to your .bashrc and run it with source
	export HIVE_HOME=/home/<USER>/hive
	export PATH=$PATH:$HIVE_HOME/bin

Hadoop setup: make sure the directories are correct.
1.Add the following to hadoop-env.sh
	export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
	export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/home/<USER>/hadoop/etc/hadoop"}
2.Replace the file core-site.xml with the following:
	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	<property>
	<name>fs.default.name</name>
	<value>hdfs://localhost:9000</value>
	</property>
	</configuration>
	Replcae the file hdfs-site.xml with the following:
	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	<property>
	<name>dfs.replication</name>
	<value>1</value>
	</property>
	<property>
	<name>dfs.permission</name>
	<value>false</value>
	</property>
	</configuration>

SSH Setup:
- password-less authentication
	~$ ssh-keygen -t rsa
	~$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
- Then, try SSH-ing into the machine (type exit to quit the SSH session and return)
	~$ ssh localhost

- Start the Hadoop cluster using:
	~$ hdfs namenode -format
	~$ start-dfs.sh
	
- Setup permissions to HDFS
	~$ hadoop fs -mkdir -p /user/hive/warehouse
	~$ hadoop fs -mkdir -p /tmp
	~$ hadoop fs -chmod g+w /user/hive/warehouse
	~$ hadoop fs -chmod g+w /tmp
	
Setup Hive: 
1.Inside ~/hive/conf/ , create/edit hive-env.sh and add the following
	export HADOOP_HOME=/home/<USER>/hadoop
	export HADOOP_HEAPSIZE=512
	export HIVE_CONF_DIR=/home/<USER>/hive/conf
2.While still in ~/hive/conf , create/edit hive-site.xml and add the following. Change in the xml to point to the right location.
	<?xml version="1.0" encoding="UTF-8" standalone="no"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	<property>
	<name>javax.jdo.option.ConnectionURL</name>
	<value>jdbc:derby:;databaseName=/home/<USER>/hive/metastore_db;create=true</value>
	<description>JDBC connect string for a JDBC metastore.</description>
	</property>
	<property>
	<name>hive.metastore.warehouse.dir</name>
	<value>/user/hive/warehouse</value>
	<description>location of default database for the warehouse</description>
	</property>
	<property>
	<name>hive.metastore.uris</name>
	<value>thrift://localhost:9083</value>
	<description>Thrift URI for the remote metastore.</description>
	</property>
	<property>
	<name>javax.jdo.option.ConnectionDriverName</name>
	<value>org.apache.derby.jdbc.EmbeddedDriver</value>
	<description>Driver class name for a JDBC metastore</description>
	</property>
	<property>
	<name>javax.jdo.PersistenceManagerFactoryClass</name>
	<value>org.datanucleus.api.jdo.JDOPersistenceManagerFactory</value>
	<description>class implementing the jdo persistence</description>
	</property>
	<property>
	<name>hive.server2.enable.doAs</name>
	<value>false</value>
	</property>
	</configuration>
	
- create a database schema for Hive
	~$ schematool -initSchema -dbType derby
	
- Start the Hive Metastore server.
	~$ hive --service metastore
	
- Enter Hive shell and Make the database for storing our House Price Prediction data
	~$ hive
	...
	hive>
- Make the database for storing our House Price Prediction data
	CREATE TABLE HousePricePredictor(MSSubClass FLOAT, MSZoning STRING, LotFrontage FLOAT,
    LotArea FLOAT,
    Street STRING,
    Alley STRING,
    LotShape STRING,
    LandContour STRING,
    Utilities STRING,
    LotConfig STRING,
    LandSlope STRING,
    Neighborhood STRING,
    Condition1 STRING,
    Condition2 STRING,
    BldgType STRING,
    HouseStyle STRING,
    OverallQual FLOAT,
    OverallCond FLOAT,
    YearBuilt FLOAT,
    YearRemodAdd FLOAT,
    RoofStyle STRING,
    RoofMatl STRING,
    Exterior1st STRING,
    Exterior2nd STRING,
    MasVnrType STRING,
    MasVnrArea FLOAT,
    ExterQual STRING,
    ExterCond STRING,
    Foundation STRING,
    BsmtQual STRING,
    BsmtCond STRING,
    BsmtExposure STRING,
    BsmtFinType1 STRING,
    BsmtFinSF1 FLOAT,
    BsmtFinType2 STRING,
    BsmtFinSF2 FLOAT,
    BsmtUnfSF FLOAT,
    TotalBsmtSF FLOAT,
    Heating STRING,
    HeatingQC STRING,
    CentralAir STRING,
    Electrical STRING,
    FirstFlrSF FLOAT,
    SecondFlrSF FLOAT,
    LowQualFinSF FLOAT,
    GrLivArea FLOAT,
    BsmtFullBath FLOAT,
    BsmtHalfBath FLOAT,
    FullBath FLOAT,
    HalfBath FLOAT,
    BedroomAbvGr FLOAT,
    KitchenAbvGr FLOAT,
    KitchenQual STRING,
    TotRmsAbvGrd FLOAT,
    Functional STRING,
    Fireplaces FLOAT,
    FireplaceQu STRING,
    GarageType STRING,
    GarageYrBlt FLOAT,
    GarageFinish STRING,
    GarageCars FLOAT,
    GarageArea FLOAT,
    GarageQual STRING,
    GarageCond STRING,
    PavedDrive STRING,
    WoodDeckSF FLOAT,
    OpenPorchSF FLOAT,
    EnclosedPorch FLOAT,
    ThreeSsnPorch FLOAT,
    ScreenPorch FLOAT,
    PoolArea FLOAT,
    PoolQC STRING,
    Fence STRING,
    MiscFeature STRING,
    MiscVal FLOAT,
    MoSold FLOAT,
    YrSold FLOAT,
    SaleType STRING,
    SaleCondition STRING, SalePrice FLOAT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\|'
STORED AS TEXTFILE;

- Update spark, pyspark environment variables. Now we need to add the Spark /bin files to the path, so open up .bashrc and add the following
	export PATH=$PATH:/home/<USER>/spark/bin
	export PYSPARK_PYTHON=python3
